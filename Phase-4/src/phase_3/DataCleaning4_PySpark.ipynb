{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-reading the files that were generaed by the previous ipynb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all the libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, lower, isnan, count\n",
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3rd spark session to clean the data\n",
    "spark = SparkSession.builder.appName(\"DataCleaning4\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the previously written data\n",
    "df = spark.read.option(\"header\", True).csv(\"Intermediate3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 E. Filling values for ZIP CODE, BOROUGH -> Using Nominatam Local Instance to Fill the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üö® DO NOT RUN THIS THE FOLLOWING CELL üö®**  \n",
    "‚ö†Ô∏è This operation requires a local nominatim instance running in a docker contrainer for make queries. ‚ö†Ô∏è  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get ZIP code and borough from latitude and longitude using geopy\n",
    "geolocator = Nominatim(user_agent=\"geoapiExercises\", domain=\"localhost:8080\", scheme=\"http\")\n",
    "\n",
    "def get_location_info(lat, lon):\n",
    "    try:\n",
    "        location = geolocator.reverse((lat, lon), exactly_one=True, timeout=20)\n",
    "        if location and 'postcode' in location.raw['address']:\n",
    "            postcode = location.raw['address']['postcode']\n",
    "            borough = location.raw['address'].get('borough', 'Unknown')\n",
    "            return postcode, borough\n",
    "        else:\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the function to all the data\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def get_zipcode(lat, lon):\n",
    "    return get_location_info(lat, lon)[0]\n",
    "\n",
    "def get_borough(lat, lon):\n",
    "    return get_location_info(lat, lon)[1]\n",
    "\n",
    "get_zipcode_udf = udf(get_zipcode, StringType())\n",
    "get_borough_udf = udf(get_borough, StringType())\n",
    "\n",
    "df = df.withColumn(\"ZIP CODE\", when((col(\"ZIP CODE\") == 0) & col(\"LATITUDE\").isNotNull() & col(\"LONGITUDE\").isNotNull(), get_zipcode_udf(col(\"LATITUDE\"), col(\"LONGITUDE\"))).otherwise(col(\"ZIP CODE\")))\n",
    "df = df.withColumn(\"BOROUGH\", when((col(\"ZIP CODE\") == 0) & col(\"LATITUDE\").isNotNull() & col(\"LONGITUDE\").isNotNull(), get_borough_udf(col(\"LATITUDE\"), col(\"LONGITUDE\"))).otherwise(col(\"BOROUGH\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing information like ZIP CODE with 0, BOROUGH with Unknown\n",
    "df = df.fillna({\"ZIP CODE\": 0, \"BOROUGH\": \"Unknown\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make all the borough data to lowercase\n",
    "df = df.withColumn(\"BOROUGH\", lower(col(\"BOROUGH\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing all the incorrect BOROUGH names\n",
    "df = df.withColumn(\"BOROUGH\", when(col(\"BOROUGH\") == \"the bronx\", \"bronx\").otherwise(col(\"BOROUGH\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the ZIP CODE to 0 for the rows that do not liw in new york\n",
    "df = df.withColumn(\"ZIP CODE\", when((col(\"ZIP CODE\") < 10000) | (col(\"ZIP CODE\") > 12000), 0).otherwise(col(\"ZIP CODE\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the totally cleaned dat into a new folder called \"final_data\"\n",
    "df.write.mode(\"overwrite\").option(\"header\", True).csv(\"final_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
