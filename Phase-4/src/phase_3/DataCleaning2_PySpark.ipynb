{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Reading all the files that were written by the last ipynb file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the spark tools and libraries and some helper functions\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, udf\n",
    "import requests\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start a new spark session for second cleaning phase\n",
    "spark = SparkSession.builder.appName(\"DataCleaning2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we load the cleaned data we saved from the \"Intermediate\" folder\n",
    "df = spark.read.option(\"header\", True).csv(\"Intermediate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 A. Filling missing latitude and longitude using external library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸš¨ DO NOT RUN THIS THE FOLLOWING CELL ðŸš¨**  \n",
    "âš ï¸ This operation takes approx **21 days** to complete if running on a single PC!, threading is not allowed to this API, nor is request.session() âš ï¸  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we define the function to get latitutde, longitude and then location name from the address using the NOMINATIM API\n",
    "def fetch_location(address):\n",
    "    url = f\"https://nominatim.openstreetmap.org/search?format=json&q={address}\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Brave/91.0.4472.124'\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    if response.status_code == 200 and len(response.json()) > 0:\n",
    "        result = response.json()[0]\n",
    "        return result['lat'], result['lon'], result['display_name']\n",
    "    return None, None, None\n",
    "\n",
    "fetch_location_udf = udf(fetch_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add 3 empty columns that we will use later to fill the data that is fetched from the API\n",
    "df = df.withColumn(\"NLat\", lit(None).cast(\"string\")) \\\n",
    "       .withColumn(\"NLong\", lit(None).cast(\"string\")) \\\n",
    "       .withColumn(\"Location\", lit(None).cast(\"string\"))\n",
    "\n",
    "# Now we keep only the rows in where latitude longitude and ZIP code is missing\n",
    "df_filtered = df.filter(~(col(\"LATITUDE\").isNotNull() & col(\"LONGITUDE\").isNotNull() & col(\"ZIP CODE\").isNotNull()))\n",
    "\n",
    "# Now we have to parse all the data from the fetched data\n",
    "for row in df_filtered.collect():\n",
    "    address = row['Addresses']\n",
    "    lat, lon, location = fetch_location(address)\n",
    "    df = df.withColumn(\"NLat\", when(col(\"Addresses\") == address, lit(lat)).otherwise(col(\"NLat\"))) \\\n",
    "           .withColumn(\"NLong\", when(col(\"Addresses\") == address, lit(lon)).otherwise(col(\"NLong\"))) \\\n",
    "           .withColumn(\"Location\", when(col(\"Addresses\") == address, lit(location)).otherwise(col(\"Location\")))\n",
    "    time.sleep(1)  # We need to slwwp for 1 second as this is the rate limit from the server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we save the updated data with the new fields in the folder \"Intermediate2\"\n",
    "\n",
    "df.write.mode(\"overwrite\").option(\"header\", True).csv(\"Intermediate2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "**End Of DataCleaning2_PySpark.ipynb file**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
