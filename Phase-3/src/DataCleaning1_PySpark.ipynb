{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning with PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we import all the tools from spark that we will use\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, concat_ws, lit, regexp_replace, lower, to_timestamp, hour, count\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 22:53:53 WARN Utils: Your hostname, Huis-Surface-Laptop-3.local resolves to a loopback address: 127.0.0.1; using 192.168.1.211 instead (on interface en0)\n",
      "25/04/13 22:53:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/13 22:53:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Now we start new spark session, so that we can work with big datasets easily\n",
    "spark = SparkSession.builder.appName(\"DataCleaning1\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Here we read the CSV file into spark with headers and types are done automatically\n",
    "df = spark.read.csv('Motor_Vehicle_Collisions_-_Crashes.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- CRASH DATE: string (nullable = true)\n",
      " |-- CRASH TIME: string (nullable = true)\n",
      " |-- BOROUGH: string (nullable = true)\n",
      " |-- ZIP CODE: string (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ON STREET NAME: string (nullable = true)\n",
      " |-- CROSS STREET NAME: string (nullable = true)\n",
      " |-- OFF STREET NAME: string (nullable = true)\n",
      " |-- NUMBER OF PERSONS INJURED: string (nullable = true)\n",
      " |-- NUMBER OF PERSONS KILLED: string (nullable = true)\n",
      " |-- NUMBER OF PEDESTRIANS INJURED: integer (nullable = true)\n",
      " |-- NUMBER OF PEDESTRIANS KILLED: integer (nullable = true)\n",
      " |-- NUMBER OF CYCLIST INJURED: integer (nullable = true)\n",
      " |-- NUMBER OF CYCLIST KILLED: integer (nullable = true)\n",
      " |-- NUMBER OF MOTORIST INJURED: string (nullable = true)\n",
      " |-- NUMBER OF MOTORIST KILLED: string (nullable = true)\n",
      " |-- CONTRIBUTING FACTOR VEHICLE 1: string (nullable = true)\n",
      " |-- CONTRIBUTING FACTOR VEHICLE 2: string (nullable = true)\n",
      " |-- CONTRIBUTING FACTOR VEHICLE 3: string (nullable = true)\n",
      " |-- CONTRIBUTING FACTOR VEHICLE 4: string (nullable = true)\n",
      " |-- CONTRIBUTING FACTOR VEHICLE 5: string (nullable = true)\n",
      " |-- COLLISION_ID: integer (nullable = true)\n",
      " |-- VEHICLE TYPE CODE 1: string (nullable = true)\n",
      " |-- VEHICLE TYPE CODE 2: string (nullable = true)\n",
      " |-- VEHICLE TYPE CODE 3: string (nullable = true)\n",
      " |-- VEHICLE TYPE CODE 4: string (nullable = true)\n",
      " |-- VEHICLE TYPE CODE 5: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/13 22:54:06 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+--------+--------+----------+--------------------+-----------------+---------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|CRASH DATE|CRASH TIME| BOROUGH|ZIP CODE|LATITUDE| LONGITUDE|      ON STREET NAME|CROSS STREET NAME|OFF STREET NAME|NUMBER OF PERSONS INJURED|NUMBER OF PERSONS KILLED|NUMBER OF PEDESTRIANS INJURED|NUMBER OF PEDESTRIANS KILLED|NUMBER OF CYCLIST INJURED|NUMBER OF CYCLIST KILLED|NUMBER OF MOTORIST INJURED|NUMBER OF MOTORIST KILLED|CONTRIBUTING FACTOR VEHICLE 1|CONTRIBUTING FACTOR VEHICLE 2|CONTRIBUTING FACTOR VEHICLE 3|CONTRIBUTING FACTOR VEHICLE 4|CONTRIBUTING FACTOR VEHICLE 5|COLLISION_ID| VEHICLE TYPE CODE 1|VEHICLE TYPE CODE 2|VEHICLE TYPE CODE 3|VEHICLE TYPE CODE 4|VEHICLE TYPE CODE 5|\n",
      "+----------+----------+--------+--------+--------+----------+--------------------+-----------------+---------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|09/11/2021|      2:39|    NULL|    NULL|    NULL|      NULL|WHITESTONE EXPRES...|        20 AVENUE|           NULL|                      2.0|                     0.0|                            0|                           0|                        0|                       0|                         2|                        0|         Aggressive Drivin...|                  Unspecified|                         NULL|                         NULL|                         NULL|     4455765|               Sedan|              Sedan|               NULL|               NULL|               NULL|\n",
      "|03/26/2022|     11:45|    NULL|    NULL|    NULL|      NULL|QUEENSBORO BRIDGE...|             NULL|           NULL|                      1.0|                     0.0|                            0|                           0|                        0|                       0|                         1|                        0|            Pavement Slippery|                         NULL|                         NULL|                         NULL|                         NULL|     4513547|               Sedan|               NULL|               NULL|               NULL|               NULL|\n",
      "|11/01/2023|      1:29|BROOKLYN| 11230.0|40.62179|-73.970024|       OCEAN PARKWAY|         AVENUE K|           NULL|                      1.0|                     0.0|                            0|                           0|                        0|                       0|                         1|                        0|                  Unspecified|                  Unspecified|                  Unspecified|                         NULL|                         NULL|     4675373|               Moped|              Sedan|              Sedan|               NULL|               NULL|\n",
      "|06/29/2022|      6:55|    NULL|    NULL|    NULL|      NULL|  THROGS NECK BRIDGE|             NULL|           NULL|                      0.0|                     0.0|                            0|                           0|                        0|                       0|                         0|                        0|         Following Too Clo...|                  Unspecified|                         NULL|                         NULL|                         NULL|     4541903|               Sedan|      Pick-up Truck|               NULL|               NULL|               NULL|\n",
      "|09/21/2022|     13:21|    NULL|    NULL|    NULL|      NULL|     BROOKLYN BRIDGE|             NULL|           NULL|                      0.0|                     0.0|                            0|                           0|                        0|                       0|                         0|                        0|          Passing Too Closely|                  Unspecified|                         NULL|                         NULL|                         NULL|     4566131|Station Wagon/Spo...|               NULL|               NULL|               NULL|               NULL|\n",
      "+----------+----------+--------+--------+--------+----------+--------------------+-----------------+---------------+-------------------------+------------------------+-----------------------------+----------------------------+-------------------------+------------------------+--------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------+--------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we check the structure and take a look at first 5 entries\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Filling Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 A. Filling in the empty values with 'Unspecified' for \"CONTRIBUTING FACTOR VEHICLE 1,2,3,4,5\"\n",
    "\n",
    "# Initially we replace the values with nan and then fill it with 'Unspecified'\n",
    "contributing_factor_columns = [col for col in df.columns if 'CONTRIBUTING FACTOR VEHICLE' in col]\n",
    "for column in contributing_factor_columns:\n",
    "    df = df.withColumn(column, when(col(column).isin(['nan', 'NaN', 'None', '', ' ', 'N/A', '1', '80']), None).otherwise(col(column)))\n",
    "    df = df.fillna({column: 'Unspecified'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1B. Here we replace them missing number with 0 in all the columns with name as 'NUMBER' as their name\n",
    "number_columns = [col for col in df.columns if 'NUMBER' in col]\n",
    "for column in number_columns:\n",
    "    df = df.fillna({column: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1C. Replacing 0 values with None in 'LATITUDE', 'LONGITUDE', and 'ZIP CODE'\n",
    "df = df.withColumn('LATITUDE', when(col('LATITUDE') == 0, None).otherwise(col('LATITUDE')))\n",
    "df = df.withColumn('LONGITUDE', when(col('LONGITUDE') == 0, None).otherwise(col('LONGITUDE')))\n",
    "df = df.withColumn('ZIP CODE', when(col('ZIP CODE') == 0, None).otherwise(col('ZIP CODE')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Shifting Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2A. Filling empty vehicle types by copying the next columns value if its there\n",
    "vehicle_columns = [col for col in df.columns if 'VEHICLE TYPE CODE' in col]\n",
    "for i in range(len(vehicle_columns) - 1):\n",
    "    df = df.withColumn(vehicle_columns[i], when(col(vehicle_columns[i]).isNull(), col(vehicle_columns[i + 1])).otherwise(col(vehicle_columns[i])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Changing Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3A. Here we comine all the street names into one columns 'Addresses' with no extra spaces.\n",
    "df = df.withColumn('Addresses', concat_ws(' ', lower(col('ON STREET NAME')), lower(col('CROSS STREET NAME')), lower(col('OFF STREET NAME'))))\n",
    "df = df.withColumn('Addresses', regexp_replace(col('Addresses'), '\\s+', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3B. Here we are turning all the behicle type values in lower case \n",
    "for column in vehicle_columns:\n",
    "    df = df.withColumn(column, lower(col(column)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3C. Here we are making all the contributing factor values to lower case\n",
    "for column in contributing_factor_columns:\n",
    "    df = df.withColumn(column, lower(col(column)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fixing Date and Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we combine the crash date and crash time to a single timestamp that is consistent\n",
    "df = df.withColumn('CRASH DATE & TIME', to_timestamp(concat_ws(' ', col('CRASH DATE'), col('CRASH TIME'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Removing Duplicate Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we remove all the duplicate rows that have the same crash time and location\n",
    "df = df.dropDuplicates(['CRASH DATE & TIME', 'LATITUDE', 'LONGITUDE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating Time of Day Attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we label every crash by time of the day based on the hour of the crash took place\n",
    "df = df.withColumn('Time of Day', when((hour(col('CRASH DATE & TIME')) >= 5) & (hour(col('CRASH DATE & TIME')) < 12), 'morning')\n",
    "                           .when((hour(col('CRASH DATE & TIME')) >= 12) & (hour(col('CRASH DATE & TIME')) < 17), 'afternoon')\n",
    "                           .when((hour(col('CRASH DATE & TIME')) >= 17) & (hour(col('CRASH DATE & TIME')) < 21), 'evening')\n",
    "                           .otherwise('night'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Changing Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we convert zip code, injuries and deaths to integers \n",
    "df = df.withColumn('ZIP CODE', col('ZIP CODE').cast(IntegerType()))\n",
    "df = df.withColumn('NUMBER OF PERSONS INJURED', col('NUMBER OF PERSONS INJURED').cast(IntegerType()))\n",
    "df = df.withColumn('NUMBER OF PERSONS KILLED', col('NUMBER OF PERSONS KILLED').cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dropping Unnecessary Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the extra columns, since we already have this information in other column\n",
    "columns_to_drop = ['ON STREET NAME', 'CROSS STREET NAME', 'OFF STREET NAME', 'CRASH DATE', 'CRASH TIME']\n",
    "df = df.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now finally we move the crash date and time to the front so as to make the table easier to read.\n",
    "columns = ['CRASH DATE & TIME'] + [col for col in df.columns if col != 'CRASH DATE & TIME']\n",
    "df = df.select(*columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Java HotSpot(TM) 64-Bit Server VM warning: CodeCache is full. Compiler has been disabled.\n",
      "Java HotSpot(TM) 64-Bit Server VM warning: Try increasing the code cache size using -XX:ReservedCodeCacheSize=\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeCache: size=131072Kb used=30453Kb max_used=30747Kb free=100618Kb\n",
      " bounds [0x00000001069e0000, 0x0000000108820000, 0x000000010e9e0000]\n",
      " total_blobs=10531 nmethods=9603 adapters=840\n",
      " compilation: disabled (not enough contiguous free space left)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Now since all the processing is done we save the output to new CSV folder called \"Intermediate\"\n",
    "df.write.mode(\"overwrite\").option(\"header\", True).csv(\"Intermediate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
